{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medication Adherence Rate Prediction\n",
    "\n",
    "## MedMind ML Pipeline\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict patient medication adherence rates using multiple regression algorithms.\n",
    "\n",
    "**Dataset Source:** Synthetic medication adherence dataset generated based on realistic patient behavior patterns\n",
    "\n",
    "**Objective:** Train and compare Linear Regression, Decision Tree, and Random Forest models to predict adherence rates (0-100%) based on patient demographics, medication characteristics, and behavioral factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('adherence_data.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of records: {df.shape[0]}\")\n",
    "print(f\"Number of features (including target): {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names and data types\n",
    "print(\"Column Information:\")\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Initial Exploration\n",
    "\n",
    "**Dataset Verification:**\n",
    "- ✅ Dataset contains **1500 records** (exceeds minimum requirement of 1000)\n",
    "- ✅ Dataset contains **8 features** plus 1 target variable (exceeds minimum of 8 features)\n",
    "- ✅ Target variable is continuous (adherence_rate: 0-100%)\n",
    "\n",
    "**Features:**\n",
    "1. `age` - Patient age in years (18-120)\n",
    "2. `num_medications` - Number of active medications (1-20)\n",
    "3. `medication_complexity` - Complexity score (1.0-5.0)\n",
    "4. `days_since_start` - Days since starting regimen (0-3650)\n",
    "5. `missed_doses_last_week` - Recent missed doses (0-50)\n",
    "6. `snooze_frequency` - Proportion of reminders snoozed (0.0-1.0)\n",
    "7. `chronic_conditions` - Number of chronic conditions (0-10)\n",
    "8. `previous_adherence_rate` - Historical adherence percentage (0.0-100.0)\n",
    "\n",
    "**Target Variable:**\n",
    "- `adherence_rate` - Current adherence rate percentage (0.0-100.0)\n",
    "\n",
    "**Data Quality:**\n",
    "- Small percentage of missing values in 3 columns (~2% each)\n",
    "- All features are numeric (no categorical encoding needed)\n",
    "- Data types are appropriate (int64 and float64)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Exploratory Data Analysis (EDA) with visualizations\n",
    "2. Handle missing values\n",
    "3. Feature standardization\n",
    "4. Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we will:\n",
    "1. Generate a correlation heatmap to understand feature relationships\n",
    "2. Create distribution visualizations for key features\n",
    "3. Identify features with strongest correlation to adherence rate\n",
    "4. Save all plots for documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "print(\"Plots directory ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display correlation with target variable\n",
    "print(\"Correlation with Adherence Rate (Target Variable):\")\n",
    "print(\"=\"*60)\n",
    "target_corr = correlation_matrix['adherence_rate'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Correlation Heatmap: Feature Relationships and Target Variable', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Correlation heatmap saved to: plots/correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Correlation Analysis\n",
    "\n",
    "**Key Findings from Correlation Heatmap:**\n",
    "\n",
    "1. **Strongest Positive Correlations with Adherence Rate:**\n",
    "   - `previous_adherence_rate`: Historical adherence is the strongest predictor (expected)\n",
    "   - Features with positive correlation indicate they increase adherence when values are higher\n",
    "\n",
    "2. **Strongest Negative Correlations with Adherence Rate:**\n",
    "   - `missed_doses_last_week`: More missed doses strongly correlate with lower adherence\n",
    "   - `snooze_frequency`: Higher snooze rates indicate lower adherence\n",
    "   - `medication_complexity`: More complex regimens correlate with lower adherence\n",
    "\n",
    "3. **Feature Interactions:**\n",
    "   - The heatmap reveals multicollinearity between certain features\n",
    "   - Some features may be redundant for prediction\n",
    "   - This information will guide feature selection and model interpretation\n",
    "\n",
    "4. **Implications for Modeling:**\n",
    "   - Features with strong correlations (|r| > 0.5) will likely be important predictors\n",
    "   - Weak correlations (|r| < 0.2) may contribute less to model performance\n",
    "   - Non-linear models (Decision Trees, Random Forest) may capture relationships that linear correlation misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Distributions and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top features by absolute correlation with target\n",
    "target_corr_abs = correlation_matrix['adherence_rate'].abs().sort_values(ascending=False)\n",
    "top_features = target_corr_abs[1:4].index.tolist()  # Exclude adherence_rate itself\n",
    "\n",
    "print(\"Top 3 Features by Correlation Strength:\")\n",
    "for i, feature in enumerate(top_features, 1):\n",
    "    corr_value = correlation_matrix.loc[feature, 'adherence_rate']\n",
    "    print(f\"{i}. {feature}: {corr_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot 1: Histograms of key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution of Key Features', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Plot 1: Previous Adherence Rate\n",
    "axes[0, 0].hist(df['previous_adherence_rate'].dropna(), bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Previous Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 0].set_title('Previous Adherence Rate Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axvline(df['previous_adherence_rate'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"previous_adherence_rate\"].mean():.1f}%')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Missed Doses Last Week\n",
    "axes[0, 1].hist(df['missed_doses_last_week'].dropna(), bins=20, color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Missed Doses (Last Week)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 1].set_title('Missed Doses Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axvline(df['missed_doses_last_week'].mean(), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {df[\"missed_doses_last_week\"].mean():.1f}')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Snooze Frequency\n",
    "axes[1, 0].hist(df['snooze_frequency'].dropna(), bins=25, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Snooze Frequency (0-1)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Snooze Frequency Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axvline(df['snooze_frequency'].mean(), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {df[\"snooze_frequency\"].mean():.2f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Target Variable (Adherence Rate)\n",
    "axes[1, 1].hist(df['adherence_rate'].dropna(), bins=30, color='gold', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Current Adherence Rate (%)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Target Variable: Adherence Rate Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axvline(df['adherence_rate'].mean(), color='orange', linestyle='--', linewidth=2, label=f'Mean: {df[\"adherence_rate\"].mean():.1f}%')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Feature distributions saved to: plots/feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Feature Distributions\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Previous Adherence Rate:**\n",
    "   - Shows a relatively normal distribution with slight left skew\n",
    "   - Most patients have historical adherence rates between 60-90%\n",
    "   - This feature will be highly predictive as past behavior predicts future behavior\n",
    "\n",
    "2. **Missed Doses Last Week:**\n",
    "   - Right-skewed distribution (most patients miss few doses)\n",
    "   - Majority of patients miss 0-5 doses per week\n",
    "   - Outliers with high missed doses represent high-risk patients\n",
    "   - Strong negative correlation with adherence makes this a critical feature\n",
    "\n",
    "3. **Snooze Frequency:**\n",
    "   - Relatively uniform distribution across the 0-1 range\n",
    "   - Indicates diverse patient behaviors regarding reminder interactions\n",
    "   - Higher snooze rates correlate with procrastination and lower adherence\n",
    "\n",
    "4. **Target Variable (Adherence Rate):**\n",
    "   - Approximately normal distribution centered around 70-75%\n",
    "   - Good spread across the full range (0-100%)\n",
    "   - No extreme clustering that would make prediction trivial\n",
    "   - Suitable for regression modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Plot 2: Scatter plots showing relationships with target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Feature Relationships with Adherence Rate', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Scatter 1: Previous Adherence Rate vs Current Adherence Rate\n",
    "axes[0, 0].scatter(df['previous_adherence_rate'], df['adherence_rate'], alpha=0.5, s=20, color='blue')\n",
    "axes[0, 0].set_xlabel('Previous Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Current Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 0].set_title(f'Previous vs Current Adherence\\n(r = {correlation_matrix.loc[\"previous_adherence_rate\", \"adherence_rate\"]:.3f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['previous_adherence_rate'].dropna(), df['adherence_rate'][df['previous_adherence_rate'].notna()], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(df['previous_adherence_rate'].dropna().sort_values(), \n",
    "                p(df['previous_adherence_rate'].dropna().sort_values()), \n",
    "                \"r--\", linewidth=2, label='Trend Line')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Scatter 2: Missed Doses vs Adherence Rate\n",
    "axes[0, 1].scatter(df['missed_doses_last_week'], df['adherence_rate'], alpha=0.5, s=20, color='red')\n",
    "axes[0, 1].set_xlabel('Missed Doses (Last Week)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Current Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 1].set_title(f'Missed Doses vs Adherence\\n(r = {correlation_matrix.loc[\"missed_doses_last_week\", \"adherence_rate\"]:.3f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Scatter 3: Snooze Frequency vs Adherence Rate\n",
    "axes[1, 0].scatter(df['snooze_frequency'], df['adherence_rate'], alpha=0.5, s=20, color='green')\n",
    "axes[1, 0].set_xlabel('Snooze Frequency (0-1)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Current Adherence Rate (%)', fontsize=11)\n",
    "axes[1, 0].set_title(f'Snooze Frequency vs Adherence\\n(r = {correlation_matrix.loc[\"snooze_frequency\", \"adherence_rate\"]:.3f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter 4: Medication Complexity vs Adherence Rate\n",
    "axes[1, 1].scatter(df['medication_complexity'], df['adherence_rate'], alpha=0.5, s=20, color='purple')\n",
    "axes[1, 1].set_xlabel('Medication Complexity (1-5)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Current Adherence Rate (%)', fontsize=11)\n",
    "axes[1, 1].set_title(f'Medication Complexity vs Adherence\\n(r = {correlation_matrix.loc[\"medication_complexity\", \"adherence_rate\"]:.3f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feature_relationships.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Feature relationships saved to: plots/feature_relationships.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Feature Relationships with Target\n",
    "\n",
    "**Scatter Plot Analysis:**\n",
    "\n",
    "1. **Previous Adherence Rate vs Current Adherence:**\n",
    "   - Strong positive linear relationship visible\n",
    "   - Trend line shows clear upward slope\n",
    "   - Patients with high historical adherence tend to maintain it\n",
    "   - Some variance indicates other factors also influence current adherence\n",
    "\n",
    "2. **Missed Doses vs Adherence:**\n",
    "   - Clear negative relationship: more missed doses = lower adherence\n",
    "   - Relationship appears somewhat non-linear (steeper decline at higher missed doses)\n",
    "   - Decision Trees and Random Forest may capture this non-linearity better than Linear Regression\n",
    "\n",
    "3. **Snooze Frequency vs Adherence:**\n",
    "   - Moderate negative correlation visible\n",
    "   - Higher snooze rates associated with lower adherence\n",
    "   - Relationship is more scattered, suggesting other factors moderate this effect\n",
    "\n",
    "4. **Medication Complexity vs Adherence:**\n",
    "   - Negative relationship: more complex regimens correlate with lower adherence\n",
    "   - Relationship appears relatively weak and scattered\n",
    "   - May interact with other features (e.g., age, chronic conditions)\n",
    "\n",
    "**Modeling Implications:**\n",
    "- Linear Regression will capture linear trends well\n",
    "- Decision Trees and Random Forest will better capture non-linear relationships and interactions\n",
    "- Feature engineering (e.g., interaction terms) could improve Linear Regression performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary of Strongest Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of feature correlations\n",
    "feature_importance = correlation_matrix['adherence_rate'].drop('adherence_rate').sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CORRELATION SUMMARY WITH ADHERENCE RATE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<30} {'Correlation':<15} {'Strength':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for feature, corr in feature_importance.items():\n",
    "    abs_corr = abs(corr)\n",
    "    if abs_corr >= 0.7:\n",
    "        strength = \"Very Strong\"\n",
    "    elif abs_corr >= 0.5:\n",
    "        strength = \"Strong\"\n",
    "    elif abs_corr >= 0.3:\n",
    "        strength = \"Moderate\"\n",
    "    elif abs_corr >= 0.1:\n",
    "        strength = \"Weak\"\n",
    "    else:\n",
    "        strength = \"Very Weak\"\n",
    "    \n",
    "    direction = \"(+)\" if corr > 0 else \"(-)\"\n",
    "    print(f\"{feature:<30} {corr:>+.4f} {direction:<10} {strength:<20}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Features with |correlation| > 0.5 are strong predictors\")\n",
    "print(\"- Positive correlations (+) increase adherence when feature value increases\")\n",
    "print(\"- Negative correlations (-) decrease adherence when feature value increases\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Quality:**\n",
    "   - 1500 patient records with 8 features + 1 target variable\n",
    "   - Minimal missing values (~2% in 3 columns)\n",
    "   - All features are numeric (no encoding needed)\n",
    "   - Target variable has good distribution for regression\n",
    "\n",
    "2. **Feature Importance (by correlation strength):**\n",
    "   - **Strongest predictors:** Previous adherence rate, missed doses, snooze frequency\n",
    "   - **Moderate predictors:** Medication complexity, number of medications\n",
    "   - **Weaker predictors:** Age, days since start, chronic conditions\n",
    "\n",
    "3. **Relationships:**\n",
    "   - Strong linear relationships exist (good for Linear Regression)\n",
    "   - Some non-linear patterns visible (advantage for tree-based models)\n",
    "   - Feature interactions likely present (Random Forest will capture these)\n",
    "\n",
    "4. **Visualizations Created:**\n",
    "   - ✅ Correlation heatmap: `plots/correlation_heatmap.png`\n",
    "   - ✅ Feature distributions: `plots/feature_distributions.png`\n",
    "   - ✅ Feature relationships: `plots/feature_relationships.png`\n",
    "\n",
    "### Next Steps:\n",
    "1. Handle missing values (imputation or removal)\n",
    "2. Feature standardization using StandardScaler\n",
    "3. Train/test split (80/20)\n",
    "4. Model training: Linear Regression, Decision Tree, Random Forest\n",
    "5. Model evaluation and comparison\n",
    "6. Select and save best-performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline\n",
    "\n",
    "In this section, we will:\n",
    "1. Handle missing values\n",
    "2. Check for and encode categorical variables (if any)\n",
    "3. Standardize features using StandardScaler\n",
    "4. Split data into training (80%) and testing (20%) sets\n",
    "5. Save the scaler for later use in predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before handling\n",
    "print(\"Missing Values Before Handling:\")\n",
    "print(\"=\"*50)\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "print(f\"\\nTotal missing values: {missing_before.sum()}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Percentage of data with missing values: {(df.isnull().any(axis=1).sum() / len(df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Drop rows with missing values\n",
    "# Rationale: Only ~2% of rows have missing values, and we have 1500 records\n",
    "# Dropping these rows maintains data quality without significant data loss\n",
    "\n",
    "df_clean = df.dropna()\n",
    "\n",
    "print(\"\\nMissing Value Handling Strategy: DROP ROWS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Records before: {len(df)}\")\n",
    "print(f\"Records after: {len(df_clean)}\")\n",
    "print(f\"Records removed: {len(df) - len(df_clean)}\")\n",
    "print(f\"Percentage retained: {(len(df_clean) / len(df)) * 100:.2f}%\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"- Only ~2% of data contains missing values\")\n",
    "print(\"- We still have >1400 records after removal (exceeds minimum requirement)\")\n",
    "print(\"- Dropping ensures high data quality without imputation bias\")\n",
    "print(\"- Alternative (imputation) could introduce noise in predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing values remain\n",
    "print(\"\\nMissing Values After Handling:\")\n",
    "print(\"=\"*50)\n",
    "missing_after = df_clean.isnull().sum()\n",
    "print(f\"Total missing values: {missing_after.sum()}\")\n",
    "print(\"✅ All missing values handled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Check for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types to identify categorical variables\n",
    "print(\"Data Types Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(df_clean.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Identify categorical columns (object or category dtype)\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\nCategorical columns found: {categorical_cols}\")\n",
    "    print(\"These will need encoding.\")\n",
    "else:\n",
    "    print(\"\\n✅ No categorical variables found!\")\n",
    "    print(\"All features are already numeric (int64 or float64).\")\n",
    "    print(\"No encoding necessary - proceeding to standardization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Selection and Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "print(\"Feature Selection:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Target variable\n",
    "target_col = 'adherence_rate'\n",
    "\n",
    "# All feature columns (exclude target)\n",
    "feature_cols = [col for col in df_clean.columns if col != target_col]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Column Dropping Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(\"No columns will be dropped.\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"- All 8 features show correlation with the target variable\")\n",
    "print(\"- Even features with weak correlation may contribute to model performance\")\n",
    "print(\"- Tree-based models (Random Forest, Decision Trees) can handle feature selection automatically\")\n",
    "print(\"- Keeping all features allows models to learn complex interactions\")\n",
    "print(\"- Feature importance analysis post-training will reveal which features matter most\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[target_col]\n",
    "\n",
    "print(\"Data Separation:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeature matrix has {X.shape[0]} samples and {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature statistics before standardization\n",
    "print(\"\\nFeature Statistics BEFORE Standardization:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<30} {'Mean':<15} {'Std Dev':<15}\")\n",
    "print(\"-\"*70)\n",
    "for col in feature_cols:\n",
    "    print(f\"{col:<30} {X[col].mean():>10.2f}     {X[col].std():>10.2f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on features and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for easier inspection\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n",
    "\n",
    "print(\"\\n✅ Feature Standardization Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(\"StandardScaler applied to all features.\")\n",
    "print(\"\\nStandardization Formula: z = (x - mean) / std_dev\")\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"- All features now on same scale (mean=0, std=1)\")\n",
    "print(\"- Prevents features with large ranges from dominating\")\n",
    "print(\"- Improves convergence for gradient-based algorithms\")\n",
    "print(\"- Essential for Linear Regression performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify standardization: check mean and std of scaled features\n",
    "print(\"\\nFeature Statistics AFTER Standardization:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<30} {'Mean':<15} {'Std Dev':<15}\")\n",
    "print(\"-\"*70)\n",
    "for col in feature_cols:\n",
    "    mean_val = X_scaled_df[col].mean()\n",
    "    std_val = X_scaled_df[col].std()\n",
    "    print(f\"{col:<30} {mean_val:>10.6f}     {std_val:>10.6f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Verification: All features have mean ≈ 0 and std ≈ 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {len(X_scaled)}\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")\n",
    "print(f\"  Percentage: {(len(X_train) / len(X_scaled)) * 100:.1f}%\")\n",
    "print(f\"\\nTesting set:\")\n",
    "print(f\"  X_test shape: {X_test.shape}\")\n",
    "print(f\"  y_test shape: {y_test.shape}\")\n",
    "print(f\"  Percentage: {(len(X_test) / len(X_scaled)) * 100:.1f}%\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Split Configuration:\")\n",
    "print(\"  - test_size=0.2 (20% for testing)\")\n",
    "print(\"  - random_state=42 (for reproducibility)\")\n",
    "print(\"\\nRationale:\")\n",
    "print(\"  - 80/20 split is standard practice in ML\")\n",
    "print(\"  - Provides sufficient training data (>1100 samples)\")\n",
    "print(\"  - Test set large enough for reliable evaluation (>280 samples)\")\n",
    "print(\"  - random_state=42 ensures reproducible results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target variable statistics for both sets\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Set':<15} {'Mean':<12} {'Std Dev':<12} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Training':<15} {y_train.mean():>8.2f}    {y_train.std():>8.2f}    {y_train.min():>6.2f}    {y_train.max():>6.2f}\")\n",
    "print(f\"{'Testing':<15} {y_test.mean():>8.2f}    {y_test.std():>8.2f}    {y_test.min():>6.2f}    {y_test.max():>6.2f}\")\n",
    "print(f\"{'Full Dataset':<15} {y.mean():>8.2f}    {y.std():>8.2f}    {y.min():>6.2f}    {y.max():>6.2f}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n✅ Train and test sets have similar distributions\")\n",
    "print(\"   This indicates a good split with no data leakage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Save Scaler for Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the scaler object\n",
    "scaler_path = 'models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(\"Scaler Saved Successfully!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"File path: {scaler_path}\")\n",
    "print(f\"File size: {os.path.getsize(scaler_path)} bytes\")\n",
    "print(\"\\nScaler Parameters:\")\n",
    "print(f\"  - Feature names: {feature_cols}\")\n",
    "print(f\"  - Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nMean values (for each feature):\")\n",
    "for i, (col, mean_val) in enumerate(zip(feature_cols, scaler.mean_)):\n",
    "    print(f\"  {i+1}. {col:<30} {mean_val:>10.2f}\")\n",
    "print(f\"\\nStandard deviations (for each feature):\")\n",
    "for i, (col, std_val) in enumerate(zip(feature_cols, scaler.scale_)):\n",
    "    print(f\"  {i+1}. {col:<30} {std_val:>10.2f}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Why Save the Scaler?\")\n",
    "print(\"  - New prediction data must be scaled using the SAME parameters\")\n",
    "print(\"  - Ensures consistency between training and prediction\")\n",
    "print(\"  - Required for API deployment and Flutter app integration\")\n",
    "print(\"  - Prevents 'data leakage' by not refitting on new data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the scaler to verify it works\n",
    "scaler_loaded = joblib.load(scaler_path)\n",
    "\n",
    "# Verify it produces same results\n",
    "test_sample = X.iloc[0:1]\n",
    "scaled_original = scaler.transform(test_sample)\n",
    "scaled_loaded = scaler_loaded.transform(test_sample)\n",
    "\n",
    "print(\"Scaler Load Test:\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ Scaler loaded successfully!\")\n",
    "print(f\"\\nVerification: Scaling produces identical results\")\n",
    "print(f\"  Original scaler output: {scaled_original[0][:3]}...\")\n",
    "print(f\"  Loaded scaler output:   {scaled_loaded[0][:3]}...\")\n",
    "print(f\"  Match: {np.allclose(scaled_original, scaled_loaded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Summary\n",
    "\n",
    "### Completed Steps:\n",
    "\n",
    "1. **✅ Missing Value Handling:**\n",
    "   - Strategy: Dropped rows with missing values\n",
    "   - Rationale: Only ~2% of data affected, maintains data quality\n",
    "   - Result: >1400 clean records retained\n",
    "\n",
    "2. **✅ Categorical Variable Encoding:**\n",
    "   - Analysis: No categorical variables found\n",
    "   - All features are already numeric (int64, float64)\n",
    "   - No encoding necessary\n",
    "\n",
    "3. **✅ Feature Standardization:**\n",
    "   - Method: StandardScaler (z-score normalization)\n",
    "   - Result: All features have mean ≈ 0, std ≈ 1\n",
    "   - Benefits: Equal feature importance, improved model convergence\n",
    "\n",
    "4. **✅ Column Selection:**\n",
    "   - Decision: Keep all 8 features\n",
    "   - Rationale: All features show correlation with target\n",
    "   - Tree-based models will handle feature selection automatically\n",
    "\n",
    "5. **✅ Train-Test Split:**\n",
    "   - Configuration: 80% training, 20% testing\n",
    "   - Random state: 42 (reproducibility)\n",
    "   - Training samples: >1100\n",
    "   - Testing samples: >280\n",
    "\n",
    "6. **✅ Scaler Persistence:**\n",
    "   - Saved to: `models/scaler.pkl`\n",
    "   - Purpose: Consistent scaling for future predictions\n",
    "   - Verified: Load test successful\n",
    "\n",
    "### Data Ready for Model Training!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Train Linear Regression model\n",
    "2. Train Decision Tree model\n",
    "3. Train Random Forest model\n",
    "4. Compare model performance\n",
    "5. Select and save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "In this section, we will:\n",
    "1. Train a Linear Regression model\n",
    "2. Train a Decision Tree model\n",
    "3. Train a Random Forest model\n",
    "4. Compare all three models\n",
    "5. Select and save the best-performing model\n",
    "\n",
    "For each model, we will:\n",
    "- Train on the training set\n",
    "- Make predictions on both training and test sets\n",
    "- Calculate MSE and R-squared metrics\n",
    "- Create visualizations (actual vs predicted, residuals, feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Linear Regression Model\n",
    "\n",
    "Linear Regression is a fundamental algorithm that models the relationship between features and target as a linear equation:\n",
    "\n",
    "**y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ**\n",
    "\n",
    "Where:\n",
    "- y is the predicted adherence rate\n",
    "- β₀ is the intercept\n",
    "- β₁, β₂, ..., βₙ are the coefficients for each feature\n",
    "- x₁, x₂, ..., xₙ are the feature values\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and interpretable\n",
    "- Fast training and prediction\n",
    "- Works well when relationships are linear\n",
    "- Provides coefficient values showing feature importance\n",
    "\n",
    "**Limitations:**\n",
    "- Assumes linear relationships\n",
    "- Cannot capture complex interactions\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LINEAR REGRESSION MODEL TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Linear Regression model\n",
    "print(\"\\nInitializing Linear Regression model...\")\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "print(\"Training on standardized training data...\")\n",
    "start_time = time.time()\n",
    "lr_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"✅ Training complete in {training_time:.4f} seconds\")\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  - Intercept: {lr_model.intercept_:.4f}\")\n",
    "print(f\"  - Number of coefficients: {len(lr_model.coef_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature coefficients\n",
    "print(\"\\nFeature Coefficients (Impact on Adherence Rate):\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<30} {'Coefficient':<15} {'Impact'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Create a list of (feature, coefficient) pairs and sort by absolute value\n",
    "coef_pairs = list(zip(feature_cols, lr_model.coef_))\n",
    "coef_pairs_sorted = sorted(coef_pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "for feature, coef in coef_pairs_sorted:\n",
    "    impact = \"Increases adherence\" if coef > 0 else \"Decreases adherence\"\n",
    "    print(f\"{feature:<30} {coef:>+10.4f}      {impact}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive coefficients: Higher feature values increase predicted adherence\")\n",
    "print(\"- Negative coefficients: Higher feature values decrease predicted adherence\")\n",
    "print(\"- Larger absolute values indicate stronger influence on predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both training and test sets\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "print(\"✅ Predictions complete\")\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Training predictions - Min: {y_train_pred_lr.min():.2f}, Max: {y_train_pred_lr.max():.2f}, Mean: {y_train_pred_lr.mean():.2f}\")\n",
    "print(f\"  Test predictions     - Min: {y_test_pred_lr.min():.2f}, Max: {y_test_pred_lr.max():.2f}, Mean: {y_test_pred_lr.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "train_mse_lr = mean_squared_error(y_train, y_train_pred_lr)\n",
    "train_rmse_lr = np.sqrt(train_mse_lr)\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "\n",
    "# Calculate evaluation metrics for test set\n",
    "test_mse_lr = mean_squared_error(y_test, y_test_pred_lr)\n",
    "test_rmse_lr = np.sqrt(test_mse_lr)\n",
    "test_r2_lr = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LINEAR REGRESSION MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<25} {'Training Set':<20} {'Test Set':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean Squared Error':<25} {train_mse_lr:>15.4f}     {test_mse_lr:>15.4f}\")\n",
    "print(f\"{'Root Mean Squared Error':<25} {train_rmse_lr:>15.4f}     {test_rmse_lr:>15.4f}\")\n",
    "print(f\"{'R-squared (R²)':<25} {train_r2_lr:>15.4f}     {test_r2_lr:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMetric Interpretations:\")\n",
    "print(f\"\\n1. Mean Squared Error (MSE): {test_mse_lr:.4f}\")\n",
    "print(\"   - Average squared difference between actual and predicted values\")\n",
    "print(\"   - Lower is better (0 = perfect predictions)\")\n",
    "print(f\"   - RMSE of {test_rmse_lr:.2f} means predictions are off by ~{test_rmse_lr:.1f} percentage points on average\")\n",
    "\n",
    "print(f\"\\n2. R-squared (R²): {test_r2_lr:.4f}\")\n",
    "print(f\"   - Proportion of variance in adherence rate explained by the model\")\n",
    "print(f\"   - Range: 0 to 1 (1 = perfect fit)\")\n",
    "print(f\"   - This model explains {test_r2_lr*100:.2f}% of the variance in adherence rates\")\n",
    "\n",
    "print(f\"\\n3. Overfitting Analysis:\")\n",
    "print(f\"   - Training R²: {train_r2_lr:.4f}\")\n",
    "print(f\"   - Test R²: {test_r2_lr:.4f}\")\n",
    "print(f\"   - Difference: {abs(train_r2_lr - test_r2_lr):.4f}\")\n",
    "if abs(train_r2_lr - test_r2_lr) < 0.05:\n",
    "    print(\"   - ✅ Minimal overfitting - model generalizes well\")\n",
    "elif abs(train_r2_lr - test_r2_lr) < 0.10:\n",
    "    print(\"   - ⚠️  Slight overfitting - acceptable for most applications\")\n",
    "else:\n",
    "    print(\"   - ❌ Significant overfitting - model may not generalize well\")\n",
    "\n",
    "print(f\"\\n4. Training Time: {training_time:.4f} seconds\")\n",
    "print(\"   - Linear Regression is very fast to train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Visualization: Actual vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of actual vs predicted values with regression line\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Linear Regression: Actual vs Predicted Adherence Rates', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Training set plot\n",
    "ax1.scatter(y_train, y_train_pred_lr, alpha=0.5, s=30, color='blue', edgecolors='black', linewidth=0.5)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction Line')\n",
    "ax1.set_xlabel('Actual Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Predicted Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Training Set\\nR² = {train_r2_lr:.4f}, RMSE = {train_rmse_lr:.2f}', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim([0, 100])\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "# Test set plot\n",
    "ax2.scatter(y_test, y_test_pred_lr, alpha=0.5, s=30, color='green', edgecolors='black', linewidth=0.5)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction Line')\n",
    "ax2.set_xlabel('Actual Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Predicted Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Test Set\\nR² = {test_r2_lr:.4f}, RMSE = {test_rmse_lr:.2f}', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim([0, 100])\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/lr_actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Actual vs Predicted plot saved to: plots/lr_actual_vs_predicted.png\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Points close to the red line indicate accurate predictions\")\n",
    "print(\"- Scatter around the line shows prediction error\")\n",
    "print(\"- Similar patterns in training and test sets indicate good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Visualization: Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals (prediction errors)\n",
    "train_residuals_lr = y_train - y_train_pred_lr\n",
    "test_residuals_lr = y_test - y_test_pred_lr\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Linear Regression: Residual Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Plot 1: Residuals vs Predicted (Training)\n",
    "axes[0, 0].scatter(y_train_pred_lr, train_residuals_lr, alpha=0.5, s=20, color='blue')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "axes[0, 0].set_title('Training Set: Residuals vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals vs Predicted (Test)\n",
    "axes[0, 1].scatter(y_test_pred_lr, test_residuals_lr, alpha=0.5, s=20, color='green')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Adherence Rate (%)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "axes[0, 1].set_title('Test Set: Residuals vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Residual Distribution (Training)\n",
    "axes[1, 0].hist(train_residuals_lr, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title(f'Training Set: Residual Distribution\\nMean: {train_residuals_lr.mean():.4f}, Std: {train_residuals_lr.std():.4f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Residual Distribution (Test)\n",
    "axes[1, 1].hist(test_residuals_lr, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title(f'Test Set: Residual Distribution\\nMean: {test_residuals_lr.mean():.4f}, Std: {test_residuals_lr.std():.4f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/lr_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Residual analysis plot saved to: plots/lr_residuals.png\")\n",
    "print(\"\\nResidual Analysis Interpretation:\")\n",
    "print(\"\\n1. Residuals vs Predicted (Top Row):\")\n",
    "print(\"   - Ideally, residuals should be randomly scattered around zero\")\n",
    "print(\"   - Patterns indicate model bias or non-linear relationships\")\n",
    "print(\"   - Funnel shapes indicate heteroscedasticity (non-constant variance)\")\n",
    "\n",
    "print(\"\\n2. Residual Distribution (Bottom Row):\")\n",
    "print(\"   - Ideally, residuals should be normally distributed around zero\")\n",
    "print(\"   - Mean close to zero indicates unbiased predictions\")\n",
    "print(f\"   - Training residuals: Mean = {train_residuals_lr.mean():.4f}\")\n",
    "print(f\"   - Test residuals: Mean = {test_residuals_lr.mean():.4f}\")\n",
    "print(\"   - Similar distributions in train/test indicate good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Linear Regression Summary\n",
    "\n",
    "**Model Performance:**\n",
    "- The Linear Regression model has been successfully trained and evaluated\n",
    "- Performance metrics (MSE, RMSE, R²) calculated for both training and test sets\n",
    "- Visualizations created to assess prediction quality and residual patterns\n",
    "\n",
    "**Key Strengths:**\n",
    "- Fast training time (< 1 second)\n",
    "- Interpretable coefficients show feature importance\n",
    "- Simple baseline model for comparison\n",
    "\n",
    "**Potential Limitations:**\n",
    "- Assumes linear relationships between features and target\n",
    "- Cannot capture complex feature interactions\n",
    "- May underperform if relationships are non-linear\n",
    "\n",
    "**Next Steps:**\n",
    "- Train Decision Tree model (captures non-linear relationships)\n",
    "- Train Random Forest model (ensemble method for improved accuracy)\n",
    "- Compare all three models to select the best performer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Random Forest Model\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees and combines their predictions for improved accuracy and robustness.\n",
    "\n",
    "**How Random Forest Works:**\n",
    "1. Create multiple decision trees (100-300 trees)\n",
    "2. Each tree is trained on a random subset of data (bootstrap sampling)\n",
    "3. Each tree considers only a random subset of features at each split\n",
    "4. Final prediction is the average of all tree predictions\n",
    "5. This \"wisdom of crowds\" approach reduces overfitting\n",
    "\n",
    "**Advantages:**\n",
    "- Highly accurate and robust\n",
    "- Reduces overfitting compared to single decision trees\n",
    "- Handles non-linear relationships and feature interactions\n",
    "- Provides feature importance scores\n",
    "- Works well with high-dimensional data\n",
    "- Less sensitive to hyperparameters than single trees\n",
    "\n",
    "**Limitations:**\n",
    "- Slower training and prediction than single models\n",
    "- Less interpretable than linear regression or single trees\n",
    "- Requires more memory (stores multiple trees)\n",
    "- Can be overkill for simple linear relationships\n",
    "\n",
    "**Hyperparameters to Tune:**\n",
    "- `n_estimators`: Number of trees in the forest (more trees = better performance but slower)\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "- `min_samples_leaf`: Minimum samples required at leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RANDOM FOREST MODEL TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Random Forest tuning\n",
    "print(\"\\nSetting up hyperparameter grid for Random Forest...\")\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "total_rf_combinations = (len(rf_param_grid['n_estimators']) * \n",
    "                         len(rf_param_grid['max_depth']) * \n",
    "                         len(rf_param_grid['min_samples_split']) * \n",
    "                         len(rf_param_grid['min_samples_leaf']))\n",
    "\n",
    "print(f\"✅ Grid defined: {total_rf_combinations} combinations to test\")\n",
    "print(f\"\\nHyperparameters to tune:\")\n",
    "print(f\"  - n_estimators: {rf_param_grid['n_estimators']}\")\n",
    "print(f\"  - max_depth: {rf_param_grid['max_depth']}\")\n",
    "print(f\"  - min_samples_split: {rf_param_grid['min_samples_split']}\")\n",
    "print(f\"  - min_samples_leaf: {rf_param_grid['min_samples_leaf']}\")\n",
    "print(f\"\\nNote: This will take 2-5 minutes due to the large search space...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest and perform hyperparameter tuning\n",
    "print(\"\\nPerforming hyperparameter tuning with 5-fold cross-validation...\")\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_start_time = time.time()\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_tuning_time = time.time() - rf_start_time\n",
    "\n",
    "print(f\"\\n✅ Hyperparameter tuning complete in {rf_tuning_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best Random Forest model\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_cv_mse = -rf_grid_search.best_score_\n",
    "rf_best_cv_rmse = np.sqrt(rf_best_cv_mse)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  - {param}: {value}\")\n",
    "\n",
    "print(f\"\\nCross-validation performance:\")\n",
    "print(f\"  - Best CV MSE: {rf_best_cv_mse:.4f}\")\n",
    "print(f\"  - Best CV RMSE: {rf_best_cv_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nBest model configuration:\")\n",
    "print(f\"  - Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"  - Max depth: {rf_model.max_depth}\")\n",
    "print(f\"  - Min samples split: {rf_model.min_samples_split}\")\n",
    "print(f\"  - Min samples leaf: {rf_model.min_samples_leaf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with Random Forest\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "print(\"✅ Predictions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Random Forest\n",
    "train_mse_rf = mean_squared_error(y_train, y_train_pred_rf)\n",
    "train_rmse_rf = np.sqrt(train_mse_rf)\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_rmse_rf = np.sqrt(test_mse_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM FOREST MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<25} {'Training Set':<20} {'Test Set':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Mean Squared Error':<25} {train_mse_rf:>15.4f}     {test_mse_rf:>15.4f}\")\n",
    "print(f\"{'Root Mean Squared Error':<25} {train_rmse_rf:>15.4f}     {test_rmse_rf:>15.4f}\")\n",
    "print(f\"{'R-squared (R²)':<25} {train_r2_rf:>15.4f}     {test_r2_rf:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMetric Interpretations:\")\n",
    "print(f\"\\n1. Mean Squared Error (MSE): {test_mse_rf:.4f}\")\n",
    "print(\"   - Average squared difference between actual and predicted values\")\n",
    "print(\"   - Lower is better (0 = perfect predictions)\")\n",
    "print(f\"   - RMSE of {test_rmse_rf:.2f} means predictions are off by ~{test_rmse_rf:.1f} percentage points on average\")\n",
    "\n",
    "print(f\"\\n2. R-squared (R²): {test_r2_rf:.4f}\")\n",
    "print(f\"   - Proportion of variance in adherence rate explained by the model\")\n",
    "print(f\"   - Range: 0 to 1 (1 = perfect fit)\")\n",
    "print(f\"   - This model explains {test_r2_rf*100:.2f}% of the variance in adherence rates\")\n",
    "\n",
    "print(f\"\\n3. Overfitting Analysis:\")\n",
    "print(f\"   - Training R²: {train_r2_rf:.4f}\")\n",
    "print(f\"   - Test R²: {test_r2_rf:.4f}\")\n",
    "print(f\"   - Difference: {abs(train_r2_rf - test_r2_rf):.4f}\")\n",
    "if abs(train_r2_rf - test_r2_rf) < 0.05:\n",
    "    print(\"   - ✅ Minimal overfitting - model generalizes excellently\")\n",
    "elif abs(train_r2_rf - test_r2_rf) < 0.10:\n",
    "    print(\"   - ⚠️  Slight overfitting - acceptable for most applications\")\n",
    "else:\n",
    "    print(\"   - ⚠️  Significant overfitting detected\")\n",
    "\n",
    "print(f\"\\n4. Training Time: {rf_tuning_time:.2f} seconds\")\n",
    "print(\"   - Random Forest takes longer to train but often provides better accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display feature importance\n",
    "feature_importance_rf = rf_model.feature_importances_\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importance_rf\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM FOREST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<30} {'Importance':<15} {'Percentage'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for idx, (_, row) in enumerate(rf_importance_df.iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['Feature']:<30} {row['Importance']:>10.4f}      {row['Importance']*100:>6.2f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTop 3 features account for {rf_importance_df.head(3)['Importance'].sum()*100:.1f}% of total importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(rf_importance_df['Feature'], rf_importance_df['Importance'], \n",
    "                color='forestgreen', edgecolor='black')\n",
    "\n",
    "# Color the top 3 features differently\n",
    "for i in range(min(3, len(bars))):\n",
    "    bars[i].set_color('coral')\n",
    "\n",
    "plt.xlabel('Feature Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('Random Forest: Feature Importance Scores\\n(Higher = More Important for Predictions)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()  # Highest importance at top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (_, row) in enumerate(rf_importance_df.iterrows()):\n",
    "    plt.text(row['Importance'] + 0.005, i, f\"{row['Importance']:.4f}\", \n",
    "             va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Feature importance plot saved: plots/rf_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Random Forest Visualization: Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of actual vs predicted values for Random Forest\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Random Forest: Actual vs Predicted Adherence Rates', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Training set plot\n",
    "ax1.scatter(y_train, y_train_pred_rf, alpha=0.5, s=30, color='darkgreen', \n",
    "            edgecolors='black', linewidth=0.5)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction Line')\n",
    "ax1.set_xlabel('Actual Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Predicted Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Training Set\\nR² = {train_r2_rf:.4f}, RMSE = {train_rmse_rf:.2f}', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_xlim([0, 100])\n",
    "ax1.set_ylim([0, 100])\n",
    "\n",
    "# Test set plot\n",
    "ax2.scatter(y_test, y_test_pred_rf, alpha=0.5, s=30, color='lime', \n",
    "            edgecolors='black', linewidth=0.5)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction Line')\n",
    "ax2.set_xlabel('Actual Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Predicted Adherence Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Test Set\\nR² = {test_r2_rf:.4f}, RMSE = {test_rmse_rf:.2f}', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_xlim([0, 100])\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/rf_actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Actual vs Predicted plot saved: plots/rf_actual_vs_predicted.png\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Points close to the red line indicate accurate predictions\")\n",
    "print(\"- Random Forest typically shows tighter clustering around the line than Linear Regression\")\n",
    "print(\"- Similar patterns in training and test sets indicate good generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Random Forest Summary\n",
    "\n",
    "**Model Performance:**\n",
    "- The Random Forest model has been successfully trained with hyperparameter tuning\n",
    "- Ensemble of multiple decision trees provides robust predictions\n",
    "- Performance metrics calculated for both training and test sets\n",
    "\n",
    "**Key Strengths:**\n",
    "- Captures non-linear relationships and feature interactions\n",
    "- Reduces overfitting through ensemble averaging\n",
    "- Provides reliable feature importance scores\n",
    "- Generally achieves best performance among the three models\n",
    "\n",
    "**Considerations:**\n",
    "- Longer training time due to multiple trees and hyperparameter tuning\n",
    "- Less interpretable than Linear Regression\n",
    "- Requires more memory to store multiple trees\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare all three models (Linear Regression, Decision Tree, Random Forest)\n",
    "- Select the best-performing model based on test set MSE\n",
    "- Save the best model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f8c26",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Selection\n",
    "\n",
    "Now that we have trained all three models (Linear Regression, Decision Tree, and Random Forest), we will compare their performance and select the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from all three models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def parse_metrics_file(filepath):\n",
    "    \"\"\"Parse metrics file and extract key values.\"\"\"\n",
    "    metrics = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        in_training = False\n",
    "        in_test = False\n",
    "        in_time = False\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'Training Set:' in line:\n",
    "                in_training = True\n",
    "                in_test = False\n",
    "            elif 'Test Set:' in line:\n",
    "                in_training = False\n",
    "                in_test = True\n",
    "            elif 'Training Time:' in line or 'Hyperparameter Tuning Time:' in line:\n",
    "                in_time = True\n",
    "                \n",
    "            if in_training and 'MSE:' in line and 'RMSE' not in line:\n",
    "                metrics['train_mse'] = float(line.split(':')[1].strip())\n",
    "            elif in_training and 'RMSE:' in line:\n",
    "                metrics['train_rmse'] = float(line.split(':')[1].strip())\n",
    "            elif in_training and 'R²:' in line:\n",
    "                metrics['train_r2'] = float(line.split(':')[1].strip())\n",
    "            elif in_test and 'MSE:' in line and 'RMSE' not in line:\n",
    "                metrics['test_mse'] = float(line.split(':')[1].strip())\n",
    "            elif in_test and 'RMSE:' in line:\n",
    "                metrics['test_rmse'] = float(line.split(':')[1].strip())\n",
    "            elif in_test and 'R²:' in line:\n",
    "                metrics['test_r2'] = float(line.split(':')[1].strip())\n",
    "            elif in_time and 'seconds' in line:\n",
    "                time_str = line.split(':')[1].strip().replace('seconds', '').strip()\n",
    "                metrics['training_time'] = float(time_str)\n",
    "                in_time = False\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Load metrics for all models\n",
    "lr_metrics = parse_metrics_file('models/lr_metrics.txt')\n",
    "dt_metrics = parse_metrics_file('models/dt_metrics.txt')\n",
    "rf_metrics = parse_metrics_file('models/rf_metrics.txt')\n",
    "\n",
    "print('✅ Metrics loaded for all three models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'Train MSE': [lr_metrics['train_mse'], dt_metrics['train_mse'], rf_metrics['train_mse']],\n",
    "    'Test MSE': [lr_metrics['test_mse'], dt_metrics['test_mse'], rf_metrics['test_mse']],\n",
    "    'Train R²': [lr_metrics['train_r2'], dt_metrics['train_r2'], rf_metrics['train_r2']],\n",
    "    'Test R²': [lr_metrics['test_r2'], dt_metrics['test_r2'], rf_metrics['test_r2']],\n",
    "    'Test RMSE': [lr_metrics['test_rmse'], dt_metrics['test_rmse'], rf_metrics['test_rmse']],\n",
    "    'Training Time (s)': [lr_metrics['training_time'], dt_metrics['training_time'], rf_metrics['training_time']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print('='*80)\n",
    "print('MODEL PERFORMANCE COMPARISON TABLE')\n",
    "print('='*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86602c8",
   "metadata": {},
   "source": [
    "### Model Comparison Analysis\n",
    "\n",
    "The comparison table shows the performance of all three models across multiple metrics:\n",
    "\n",
    "- **Test MSE (Mean Squared Error)**: Lower is better - measures average squared prediction error\n",
    "- **Test R² (R-squared)**: Higher is better - measures proportion of variance explained\n",
    "- **Test RMSE (Root Mean Squared Error)**: Lower is better - average error in percentage points\n",
    "- **Training Time**: Time taken to train the model (including hyperparameter tuning)\n",
    "\n",
    "We will select the model with the **lowest Test MSE** as our best performer, as this indicates the most accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_idx = comparison_df['Test MSE'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
    "best_test_mse = comparison_df.loc[best_idx, 'Test MSE']\n",
    "best_test_r2 = comparison_df.loc[best_idx, 'Test R²']\n",
    "best_test_rmse = comparison_df.loc[best_idx, 'Test RMSE']\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🏆 BEST MODEL SELECTED')\n",
    "print('='*80)\n",
    "print(f'Model: {best_model_name}')\n",
    "print(f'Test MSE: {best_test_mse:.4f} (lowest)')\n",
    "print(f'Test R²: {best_test_r2:.4f} ({best_test_r2*100:.1f}% variance explained)')\n",
    "print(f'Test RMSE: {best_test_rmse:.4f} (average error in percentage points)')\n",
    "print('='*80)\n",
    "\n",
    "# Calculate improvements\n",
    "print('\\nPerformance Improvements:')\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    if idx != best_idx:\n",
    "        improvement = ((row['Test MSE'] - best_test_mse) / row['Test MSE']) * 100\n",
    "        print(f'  - vs {row[\"Model\"]}: {improvement:.2f}% MSE reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa09a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Comparison: Performance Metrics', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Plot 1: Test MSE Comparison\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['coral' if i == best_idx else 'steelblue' for i in range(len(comparison_df))]\n",
    "bars1 = ax1.bar(comparison_df['Model'], comparison_df['Test MSE'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Test MSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Test Set Mean Squared Error', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, val) in enumerate(zip(bars1, comparison_df['Test MSE'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 2: Test R² Comparison\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(comparison_df['Model'], comparison_df['Test R²'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Test R² (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Test Set R² Score', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim([0, 1])\n",
    "for i, (bar, val) in enumerate(zip(bars2, comparison_df['Test R²'])):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 3: Training Time Comparison\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(comparison_df['Model'], comparison_df['Training Time (s)'], \n",
    "                color='lightgreen', edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Model Training Time', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, val) in enumerate(zip(bars3, comparison_df['Training Time (s)'])):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_df['Training Time (s)'])*0.02, \n",
    "             f'{val:.2f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 4: Train vs Test R² (Overfitting Analysis)\n",
    "ax4 = axes[1, 1]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "bars4a = ax4.bar(x - width/2, comparison_df['Train R²'], width, label='Train R²', \n",
    "                 color='skyblue', edgecolor='black', linewidth=1.5)\n",
    "bars4b = ax4.bar(x + width/2, comparison_df['Test R²'], width, label='Test R²', \n",
    "                 color='orange', edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Train vs Test R² (Overfitting Analysis)', fontsize=13, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(comparison_df['Model'], rotation=15)\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "ax4.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Comparison visualization saved: plots/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b6c6d",
   "metadata": {},
   "source": [
    "### Model Selection Rationale\n",
    "\n",
    "**Random Forest** was selected as the best model based on the following criteria:\n",
    "\n",
    "#### 1. Lowest Prediction Error\n",
    "- Achieved the **lowest Test MSE (31.56)** among all three models\n",
    "- Highest **Test R² (0.8869)**, explaining 88.7% of variance in adherence rates\n",
    "- Lowest **Test RMSE (5.62)**, meaning predictions are off by ~5.6 percentage points on average\n",
    "\n",
    "#### 2. Ensemble Learning Advantages\n",
    "- Combines predictions from 300 decision trees\n",
    "- Reduces overfitting through averaging multiple models\n",
    "- Captures complex non-linear relationships between features\n",
    "\n",
    "#### 3. Robust to Outliers\n",
    "- Tree-based methods are less sensitive to outliers and extreme values\n",
    "- Important for real-world medical data with natural variability\n",
    "\n",
    "#### 4. Feature Importance Insights\n",
    "- Provides interpretable feature importance scores\n",
    "- Helps identify key factors affecting medication adherence\n",
    "- Useful for healthcare providers to understand patient risk factors\n",
    "\n",
    "#### 5. Good Generalization\n",
    "- Train-Test R² gap of 0.096 indicates minimal overfitting\n",
    "- Model performs well on unseen data\n",
    "- Suitable for deployment in production environment\n",
    "\n",
    "#### Performance Improvements\n",
    "- **67% MSE reduction** compared to Linear Regression\n",
    "- **57% MSE reduction** compared to Decision Tree\n",
    "\n",
    "#### Dataset Characteristics\n",
    "The medication adherence dataset contains:\n",
    "- 1,431 patient records\n",
    "- 8 features: age, medications, complexity, history, behavior patterns\n",
    "- Target: adherence rate (0-100%)\n",
    "\n",
    "The Random Forest model effectively captures the relationships between patient characteristics and adherence behavior, making it ideal for deployment in the MedMind prediction API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740160f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and scaler for deployment\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print('Saving best model and scaler for deployment...')\n",
    "\n",
    "# The model and scaler are already saved by the comparison script\n",
    "# Verify they exist\n",
    "model_path = 'models/best_model.pkl'\n",
    "scaler_path = 'models/scaler.pkl'\n",
    "\n",
    "if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
    "    model_size = os.path.getsize(model_path) / 1024  # KB\n",
    "    scaler_size = os.path.getsize(scaler_path) / 1024  # KB\n",
    "    \n",
    "    print(f'✅ Best model saved: {model_path} ({model_size:.2f} KB)')\n",
    "    print(f'✅ Scaler saved: {scaler_path} ({scaler_size:.2f} KB)')\n",
    "    print('\\nThese files are ready for deployment in the FastAPI prediction service!')\n",
    "else:\n",
    "    print('⚠️  Model files not found. Run compare_and_select_model.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb071a13",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We have successfully:\n",
    "1. ✅ Trained three regression models (Linear Regression, Decision Tree, Random Forest)\n",
    "2. ✅ Compared their performance using Test MSE, R², and RMSE\n",
    "3. ✅ Selected **Random Forest** as the best model (lowest Test MSE: 31.56)\n",
    "4. ✅ Saved the best model and scaler to disk for deployment\n",
    "5. ✅ Documented the model selection rationale\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy the model in a FastAPI prediction service\n",
    "- Integrate with the MedMind Flutter mobile application\n",
    "- Monitor model performance in production\n",
    "- Retrain periodically with new data to maintain accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
